{"name":"Practical-ml","tagline":"Course project for Practical Machine learning class","body":"---\r\ntitle: \"Course project - Practical machine learning\"\r\nauthor: \"Aurimas R.\"\r\ndate: \"02/17/2015\"\r\noutput: html_document\r\n---\r\n```{r, knitr::opts_chunk$set(echo=FALSE, fig.path='figures/')}\r\n```\r\n\r\nThis reports presents the process of building a machine learning algorithm for differentiating between correct and incorrect usage of dumbbell based on data collected from wearable sensors attached to dumbbell users. First, the data collected is considered; then, its preprocessing and feature selection is discussed. Three different ML models are built and using cross-validation techniques the best model is selected. The best model uses random forests technique and achieved near 100% accuracy in out-of-sample testing.\r\n\r\n#Data overview\r\n\r\n```{r, echo=FALSE, message=FALSE}\r\nsource(\"preprocessing.R\")\r\n```\r\n\r\nThe data used to build the ML algorithms is from a study by Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, [(H. Qualitative Activity Recognition of Weight Lifting Exercises)](http://groupware.les.inf.puc-rio.br/har). Note that the original study does not include a codebook, so this report infers them from their names. The training dataset includes `r dim(o_training)[1]` observations and `r dim(o_training)[2]` variables. The variables can be grouped into the following categories:\r\n\r\n* Metadata: observation id (\"X\"), name of participant (\"user_name\")\r\n* Time-related metadata: timestamps of observations and variables indicating the sliding window of observations (`r paste(\"'\", names(o_training)[3:7], \"'\", collapse=\", \")`)\r\n* Sensor data from four sensors attached to belt, arm, forearm and dumbbell:\r\n    * Six types of data are included: yaw, roll and pitch (3-dimensional axes), acceleration (x/y/z dimensions), magnetical data (x/y/z dimensions), and gyroscope (x/y/z dimensions) data.\r\n    * For each sensor-data combination the following statistics are included: raw data, kurtosis, skenewess, maximum, minimum, amplitude, average, standard deviation and variance.\r\n* Class, indicating whether the dumbbell was used correctly (A class indicates correct lifting; classes B-E indicate different wrong lifting types; see original paper for details)\r\n\r\nThe data is quite balanced, with all classes represented relatively equally (slightly more of class A):\r\n```{r echo=FALSE}\r\ntable(o_training$classe)\r\n```\r\n\r\n#Preprocessing\r\n\r\nSeveral pre-processing steps were performed:\r\n1) All variables were converted to appropriate forms (e.g. numeric / time or date / factors)\r\n2) Variables with over 50% of missing entries were deleted (this removed `r length(missing_features)` variables)\r\n3) metadata (see data description above, #1 and #2) was excluded and a ML prediction algorithm should be generic and not depend on this data.\r\n4) Zero-variance variables were deleted (defined as variables where less than 5% of values are unique). This resulted in additional `r length(zerovar)` variables removed.\r\n5) A check for linearly related features was performed (none found).\r\n6) All numeric features were scaled to 1 standard deviation and centered around 0.\r\n\r\nThis resulted in a training dataset with `r dim(training)[1]` observations and `r dim(training)[2]` remaining features. For implementation details, we refer to the [preprocessing R script](preprocessing.R).\r\n\r\n#Analysis performed\r\n\r\nBefore a model is built, it is important to consider if all variables should be included as features. However, due to lack of domain knowledge and detailed codebook, the author of this paper had to rule out any knowledge-based selection. Instead, an ad-hoc graphical inspection was performed. As the density plots indicate, randomly selected 4 variables all show limited variations between classes. A more detailed review of correlation matrices also did not reveal clear indications for model selection. Instead, all variables will be included as features into the models.\r\n \r\n```{r, echo=FALSE}\r\n\r\nfeaturePlot(x = training[, (c(6,20,22,50)), with=F],\r\n             y = training$classe,\r\n             plot = \"density\",\r\n             scales = list(x = list(relation=\"free\"),\r\n                           y = list(relation=\"free\")),\r\n             adjust = 1.5,\r\n             pch = \"|\",\r\n             layout = c(4, 1),\r\n             auto.key = list(columns = 3))\r\n```\r\n\r\n\r\n#Model selection\r\n\r\n```{r echo=FALSE, message=FALSE}\r\nsource(\"prediction.R\")\r\n```\r\n\r\nModel selection was performed as follows:\r\n\r\n1) Three different algorithms were considered: classification trees, stochastic gradient boosting and random forests;\r\n2) Training dataset was divided into three parts: training set, cross-validation set and testing set.\r\n    * Training set will be used to train the three different models\r\n    * Cross-validation set will be used to determine the best performing algorithm (based on accuracy)\r\n    * Testing set will be used to approximate an out-of-sample accuracy\r\n3) All algorithms were tuned by using simple bootstrapping (25 resampling iterations, caret default). In particular, this means that the classification trees were tuned to 1 parameter, stochastic gradient boosting to 3 parameters and random forests to 1 parameter.\r\n\r\nThe implementation details of the model selection are presented in [a prediction R script](prediction.R). Note that model training is resource-heavy, and while the underlying code relies on `data.table` and `parallel` packages to speed up the process, the whole procedure takes ~1 hour on a modern, 8 GB RAM, 8-core processor laptop.\r\n    \r\n##Performance of three models\r\n\r\nThe three models achieved the following accuracy on the training set:\r\n```{r, echo=FALSE}\r\nprint(paste(\"Accuracy on training data - CART: \", cm_tr_rpart$overall[[\"Accuracy\"]]))\r\nprint(paste(\"Accuracy on training data - random forests: \", cm_tr_rf$overall[[\"Accuracy\"]]))\r\nprint(paste(\"Accuracy on training data - Gradient Boosting: \", cm_tr_gbm$overall[[\"Accuracy\"]]))\r\n```\r\n\r\nThe performance on the cross-validation set was as follows:\r\n```{r, echo=FALSE}\r\nprint(paste(\"Accuracy on CV data - CART: \", cm_cv1_rpart$overall[[\"Accuracy\"]]))\r\nprint(paste(\"Accuracy on CV data - random forests: \", cm_cv1_rf$overall[[\"Accuracy\"]]))\r\nprint(paste(\"Accuracy on CV data - Gradient Boosting: \", cm_cv1_gbm$overall[[\"Accuracy\"]]))\r\n```\r\n\r\nBased on the above, we selected random forests as the top performing algorithm (gbm performed perfectly, too, so the choice was random). To estimate the out-of-sample accuracy (1 - error rate), we tested its accuracy over the testing set:\r\n\r\n```{r, echo=FALSE}\r\nprint(cm_cv2_rf)\r\n```\r\n\r\nIt appears that overfitting should not be a significant issue, as the model performed nearly perfectly - just as it did in the previous datasets. Note that all three models are provided as RData files in folder [`data`](data) in case the reader would like to try them out independently.\r\n \r\n#Potential improvements\r\n\r\nWhile the above model performed perfectly, there are potential improvements to be made. In particular, the training time is very high (~30 minutes for the selected model). We investigate possibilities to reduce the number of features in the model to cut down training time. \r\n\r\nThis was done by selecting only top 5 features identified as most important by `varImp()` function.\r\n\r\n```{r}\r\nimp <- varImp(m_rf$finalModel, scale=T)\r\nn <- order(imp[,1], decreasing = T)[1:5]\r\nkey_features <- rownames(imp)[order(imp[,1], decreasing = T)[1:5]]\r\nkey_features\r\n```\r\n\r\nWe then retrained random forest model using only the top 5 features. The results were as follows:\r\n```{r}\r\ntr_selected <- tr_data[, (c(\".outcome\", f)), with=F]\r\nm_rf_s <- train(.outcome ~ ., data=tr_selected, method=\"rf\", trControl=trainControl(seeds=setSeeds(135790)))\r\ntrs_pred_rf <- predict(m_rf_s, tr_data)\r\ncm_trs_rf <- confusionMatrix(trs_pred_rf, tr_data$.outcome)\r\ncv1s_pred_rf <- predict(m_rf_s, cv1_data)\r\ncm_cv1s_rf <- confusionMatrix(cv1s_pred_rf, cv1_data$classe)\r\nprint(paste(\"Accuracy on training data - random forests (5-features): \", cm_tr2_rf$overall[[\"Accuracy\"]]))\r\nprint(paste(\"Accuracy on CV data - random forests (5-features): \", cm_cv12_rf$overall[[\"Accuracy\"]]))\r\n``` \r\n\r\nIn conclusion, if the small precision loss is acceptable, significant efficiency gains can be gained by limiting features to top 5. This already indicates that not all sensors are needed and further analysis could reveal that high accuracy may be achievable with 2 or even 1 sensor at all. This would have significant implications to actual implementation of such monitoring in real world.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}